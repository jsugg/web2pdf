#!/usr/bin/env python3
"""
This module contains a web scraper designed to recursively navigate a website starting 
from a given base URL and convert the pages to PDF files. The scraper uses Playwright
for web interaction and PDF generation. It is highly configurable, allowing the user 
to specify parameters such as the maximum depth of recursion, page size for PDFs, 
delay between requests, number of retries for failed requests, and the degree of 
concurrency. The module also includes robust error handling, logging, and signal 
handling mechanisms to gracefully shut down the scraping process if interrupted.

Note: Ensure that `web2pdf` has execute permissions.
If you are unsure, run `chmod +x web2pdf` in the project folder.

Usage Examples:

# Basic usage
./web2pdf https://example.com

# Specify output directory and maximum depth
./web2pdf https://example.com --output-dir "output" --depth 3

# Customize page size and delay between requests
./web2pdf https://example.com --page-size Letter --delay 2

# Enable verbose logging
./web2pdf https://example.com --verbose

# Advanced example
./web2pdf --depth 2 --verbose --delay 1 --retries 3 --concurrency 20  --only github.io https://langchain-ai.github.io/langgraphjs/

License: MIT License

Author: Juan Sugg Gilbert
Email: juanpedrosugg [at] gmail [dot] com
"""

import argparse
import asyncio
import logging
import os
import re
import sys
import uuid
import signal
import resource
from typing import Any, Coroutine, Dict, NoReturn, Set, List, Optional, Union
from urllib.parse import (
    ParseResult,
    urlparse,
    urlunparse,
    urlencode,
    urljoin,
    parse_qs,
)

import aiohttp
from playwright.async_api import (
    async_playwright,
    Browser,
    Page,
    Response as PlaywrightResponse,
)
from playwright._impl._errors import (
    Error as PlaywrightError,
    TimeoutError as PlaywrightTimeoutError,
)
from playwright.async_api._generated import BrowserContext
from tqdm import tqdm
from PyPDF2 import PdfMerger
import nbformat
from nbconvert import HTMLExporter


class WebScraper:
    """A class to scrape websites and convert pages to PDFs using Playwright."""

    def __init__(
        self,
        config: Dict[str, Any],
        log_level: Optional[int] = None,
        log_format: Optional[str] = None,
    ) -> None:
        """
        Initialize the WebScraper with the given configuration.

        Args:
            config (Dict[str, Any]): Configuration parameters for the scraper.
        """
        self.config: Dict[str, Any] = config
        self.allowed_domains: List[str] = config.get("allowed_domains", [])
        self.excluded_urls: Set[str] = {
            url.strip() for url in config.get("excluded_urls", [])
        }
        self.base_url: str = str(config["base_url"])
        self.pagination_enabled: bool = config.get("pagination", False)
        self.output_dir: str = config["output_dir"]
        self.max_depth: int = config["max_depth"]
        self.page_size: str = config["page_size"]
        self.delay: float = config["delay"]
        self.retries: int = config["retries"]
        self.concurrency: int = config["concurrency"]
        self.max_pages: int = config["max_pages"]
        self.timeout: int = config.get(
            "timeout", 120000
        )  # 120 seconds default
        self.origin_url: str = config["base_url"]
        self.links_only: bool = config.get("links_only", False)

        self.visited_urls: Set[str] = set()
        self.pages_processed: int = 0
        self.success_count: int = 0
        self.error_count: int = 0
        self.errored_urls: List[str] = []
        self.stop_event: asyncio.Event = asyncio.Event()

        self.browser: Browser | None = None
        self.file_descriptor_semaphore: asyncio.Semaphore = asyncio.Semaphore(
            self._calculate_max_file_descriptors()
        )
        self.page_semaphore: asyncio.Semaphore = asyncio.Semaphore(
            self.concurrency
        )

        self._setup_output_directory()
        self._setup_logging(log_level=log_level, log_format=log_format)

    def _calculate_max_file_descriptors(self) -> int:
        """Calculate the maximum number of file descriptors to use (80% of the available ones)."""
        soft_limit, _ = resource.getrlimit(resource.RLIMIT_NOFILE)
        return int(soft_limit * 0.8)

    def _setup_output_directory(self) -> None:
        """Create the output directory if it doesn't exist."""
        os.makedirs(self.output_dir, exist_ok=True)

    def _setup_logging(
        self, log_level: Optional[int] = None, log_format: Optional[str] = None
    ) -> None:
        """
        Set up logging configuration based on keyword arguments.

        This function uses keyword arguments to allow for flexible logging configuration.
        Only sets up logging if 'log_level' is provided.

        Args:
            log_level (int): Logging level to be used.
            log_format (str): Specifies the logging format. Defaults to a standard format if not provided.
        """
        _log_format: str = (
            log_format or "%(asctime)s - %(levelname)s - %(message)s"
        )
        _log_level: int = log_level or logging.INFO
        logging.basicConfig(level=_log_level, format=_log_format)

    def stop(self) -> None:
        self.stop_event.set()

    async def _create_new_page(self) -> Page | None:
        try:
            if self.browser is None:
                logging.error("Browser instance is None")
                return None
            page: Page = await self.browser.new_page()
            page.set_default_navigation_timeout(self.timeout)
            return page
        except Exception as e:
            logging.error("Error creating new page: %s", str(e))
            return None

    async def scrape_website(
        self, url: str, depth: int = 0, page: Page | None = None
    ) -> None:
        if self.stop_event.is_set():
            return

        async with self.file_descriptor_semaphore:
            parsed_url: ParseResult = urlparse(url)
            base_url: str = urlunparse(
                (
                    parsed_url.scheme,
                    parsed_url.netloc,
                    parsed_url.path,
                    "",
                    "",
                    "",
                )
            )

            if await self._should_skip_url(base_url, depth):
                return

            self.visited_urls.add(base_url)
            page_created = False

            try:
                if self.browser is None:
                    logging.error("Browser instance is None")
                    return

                if page is None:
                    page = await self._create_new_page()
                    page_created = True
                if page is None:
                    logging.error("Failed to create new page for URL: %s", url)
                    return

                await self._navigate_to_url(page, url)

                if not self.links_only or url != self.origin_url:
                    await self._convert_to_pdf(url, page)
                    self.pages_processed += 1
                    self.success_count += 1
                    self.update_progress()

                if depth < self.max_depth:
                    await self._process_links(page, base_url, depth)

            except (PlaywrightError, PlaywrightTimeoutError) as e:
                await self._handle_error(url, e)
            except Exception as e:
                logging.error(
                    "Unexpected error processing %s: %s", url, str(e)
                )
                await self._handle_error(url, e)
            finally:
                if page_created and page:
                    await page.close()

    def is_jupyter_notebook_url(self, url: str) -> bool:
        return url.endswith(".ipynb") or ".ipynb?" in url

    async def _navigate_to_url(self, page: Page, url: str) -> None:
        parsed_url: ParseResult = urlparse(url)

        for attempt in range(self.retries):
            try:
                if self.is_jupyter_notebook_url(url):
                    # Handle Jupyter notebook automatic download
                    async with aiohttp.ClientSession() as session:
                        async with session.get(url) as response:  # type: ignore
                            if response:
                                if response.status != 200:
                                    raise PlaywrightError(
                                        f"Failed to download notebook: HTTP status {response.status}"
                                    )
                                notebook_content: str = await response.text()
                                notebook: nbformat.NotebookNode = (
                                    nbformat.reads(
                                        notebook_content, as_version=4
                                    )
                                )
                                html_exporter = HTMLExporter()
                                html_content, _ = (
                                    html_exporter.from_notebook_node(notebook)
                                )
                                await page.set_content(html_content)
                                await page.wait_for_load_state(
                                    "networkidle", timeout=10000
                                )
                else:
                    # Regular navigation
                    response: Optional[PlaywrightResponse] = await page.goto(
                        url,  # Use the full URL here
                        wait_until="domcontentloaded",
                        timeout=self.timeout,
                    )

                    if response and response.ok:
                        if parsed_url.fragment or parsed_url.query:
                            await page.evaluate(
                                f"window.history.replaceState(null, '', '{url}');"
                            )
                            await page.evaluate("window.scrollTo(0, 0);")

                        try:
                            await page.wait_for_load_state(
                                "networkidle", timeout=10000
                            )
                        except PlaywrightTimeoutError:
                            logging.warning(
                                "Timeout waiting for network idle on %s", url
                            )

                        await asyncio.sleep(self.delay)
                        return

                    raise PlaywrightError(
                        f"HTTP status {response.status if response else 'Unknown'}"
                    )

            except (PlaywrightError, PlaywrightTimeoutError) as e:
                logging.warning(
                    "Attempt %d failed for %s: %s", attempt + 1, url, str(e)
                )
                if attempt == self.retries - 1:
                    logging.error(
                        "Failed to retrieve %s after %d attempts",
                        url,
                        self.retries,
                    )
                    raise
                await asyncio.sleep(2**attempt)  # Exponential backoff

    async def _convert_to_pdf(self, url: str, page: Page) -> None:
        """
        Convert the given page to a PDF file using Playwright.

        Args:
            url (str): The URL of the page being converted.
            page (Page): The Playwright Page object of the page.
        """
        title: str = await page.title()
        if not title:
            title = await self._create_title(page)
        sanitized_title: str = self._sanitize_filename(title)
        sanitized_url: str = self._sanitize_filename(url)
        unique_id: str = str(uuid.uuid4())[:8]

        filename: str = (
            f"{sanitized_title}__url_{sanitized_url}__page-id-{unique_id}.pdf"
        )
        filepath: str = os.path.join(self.output_dir, filename)

        try:
            await page.pdf(path=filepath, format=self.page_size)
            logging.info("Created PDF: %s", filepath)
        except Exception as e:
            logging.error("Error creating PDF for %s: %s", url, str(e))
            logging.error("Error details:", exc_info=True)

    async def _create_title(self, page: Page) -> str:
        """
        Create a title from h1, h2, and h3 elements, handling cases where h2 or h3 appear before h1.
        """
        original_title: str = await page.title() or ""
        title: str = ""

        # Use JavaScript to extract all h1, h2, and h3 elements efficiently
        title_parts: str = await page.evaluate(
            """
            () => {
                const parts = [];
                const headers = document.body.querySelectorAll('h1, h2, h3');
                let firstH1Index = -1;
                let lastH2BeforeH1Index = -1;
                let lastH3BeforeH1Index = -1;

                headers.forEach((header, index) => {
                    if (header.tagName === 'H1' && firstH1Index === -1) {
                        firstH1Index = index;
                    }
                    if (firstH1Index === -1) {
                        if (header.tagName === 'H2') lastH2BeforeH1Index = index;
                        if (header.tagName === 'H3') lastH3BeforeH1Index = index;
                    }
                });

                // Handle headers before first H1
                if (firstH1Index > 0) {
                    if (lastH2BeforeH1Index !== -1) {
                        parts.push(headers[lastH2BeforeH1Index].textContent.trim());
                    } else if (lastH3BeforeH1Index !== -1) {
                        for (let i = 0; i <= lastH3BeforeH1Index; i++) {
                            parts.push(headers[i].textContent.trim());
                        }
                    }
                }

                // Process the rest of the headers
                let currentH1Index = -1;
                headers.forEach((header, index) => {
                    if (index < firstH1Index) return; // Skip already processed headers

                    if (header.tagName === 'H1') {
                        if (currentH1Index !== -1) parts.push('__');
                        currentH1Index = index;
                        parts.push(header.textContent.trim());
                    } else if (currentH1Index !== -1 && header.tagName === 'H2') {
                        parts.push('_' + header.textContent.trim());
                    }
                });

                // Remove trailing separator if exists
                if (parts[parts.length - 1] === '__') {
                    parts.pop();
                }

                return parts.join('');
            }
        """
        )

        # Clean up the title
        # add the condition if title is instance of str
        if isinstance(title, str) and title.strip() != "":
            title = re.sub(
                r"\s+", "-", title_parts
            )  # Replace spaces with hyphens
            title = re.sub(
                r"[^\w\-_]", "", title
            )  # Remove any non-word characters except hyphens and underscores
            title = re.sub(
                r"_+", "_", title
            )  # Replace multiple underscores with a single one
            title = re.sub(
                r"-+", "-", title
            )  # Replace multiple hyphens with a single one
            title = title.strip(
                "_-"
            )  # Remove leading/trailing underscores and hyphens

        if isinstance(original_title, str) and original_title.strip() != "":
            "__".join((original_title, title))
        return title[
            :130
        ]  # Limit title length to 100 characters for file system compatibility

    def _join_url(self, base: str, url: str) -> str:
        joined_url: str = urljoin(base, url)
        parsed_join: ParseResult = urlparse(joined_url)
        parsed_base: ParseResult = urlparse(base)

        # Preserve query parameters from both base and joined URL
        query_params: Dict[str, List[str]] = parse_qs(parsed_base.query)
        query_params |= parse_qs(parsed_join.query)

        return urlunparse(
            (
                parsed_join.scheme,
                parsed_join.netloc,
                parsed_join.path,
                parsed_join.params,
                urlencode(query_params, doseq=True),
                parsed_join.fragment,
            )
        )

    async def _process_links(
        self, page: Page, current_url: str, depth: int
    ) -> None:
        if self.stop_event.is_set():
            return

        links: List[str] = await page.eval_on_selector_all(
            "a[href]",
            """
            (elements) => elements.map(el => el.href)
            """,
        )

        resolved_links: List[str] = [
            self._join_url(current_url, link) for link in links
        ]

        if self.pagination_enabled:
            pagination_links: List[str] = [
                link
                for link in resolved_links
                if self._is_pagination_link(link)
            ]
            other_links: List[str] = [
                link for link in resolved_links if link not in pagination_links
            ]

            for link in pagination_links:
                if not await self._should_skip_url(link, depth):
                    await self.scrape_website(link, depth)
        else:
            other_links = resolved_links

        async with self.page_semaphore:
            tasks: List[Coroutine[Any, Any, None]] = [
                self.scrape_website(link, depth + 1)
                for link in other_links
                if not await self._should_skip_url(link, depth + 1)
            ]
            await asyncio.gather(*tasks)

    def _is_pagination_link(self, url: str) -> bool:
        if not self.pagination_enabled:
            return False

        parsed_url: ParseResult = urlparse(url)
        query_params: Dict[str, List[str]] = parse_qs(parsed_url.query)

        is_allowed_domain: bool = any(
            parsed_url.netloc.endswith(domain)
            for domain in self.allowed_domains
        )
        has_page_param: bool = any(
            param.lower() == "page" for param in query_params
        )

        # Check if the URL is similar to the base URL
        parsed_base_url: ParseResult = urlparse(self.base_url)
        is_similar_to_base: bool = (
            parsed_url.scheme == parsed_base_url.scheme
            and parsed_url.netloc == parsed_base_url.netloc
            and parsed_url.path == parsed_base_url.path
        )

        return is_allowed_domain and has_page_param and is_similar_to_base

    async def _should_skip_url(self, url: str, depth: int) -> bool:
        if depth > self.max_depth:
            logging.debug(f"Skipping {url} due to depth limit")
            return True

        if url in self.visited_urls:
            logging.debug(f"Skipping {url} as it has already been visited")
            return True

        parsed_url: ParseResult = urlparse(url)
        if self.allowed_domains:
            netloc_path: str = parsed_url.netloc + str(parsed_url.path)
            logging.debug(
                f"Checking {netloc_path} against allowed domains: {self.allowed_domains}"
            )
            if not any(
                netloc_path.startswith(domain)
                for domain in self.allowed_domains
            ):
                logging.debug(
                    f"Skipping {url} as it doesn't match any allowed domain"
                )
                return True

        logging.debug(f"URL {url} passed all checks and will be processed")
        return False

    async def _handle_error(self, url: str, error: Exception) -> None:
        """
        Handle errors that occur during scraping.

        Args:
            url (str): The URL where the error occurred.
            error (Exception): The exception that was raised.
        """
        self.error_count += 1
        self.errored_urls.append(f"{url} | Error: {str(error)[:50]}...")
        logging.error("Error processing %s: %s", url, str(error))

    def _sanitize_filename(self, filename: str) -> str:
        """
        Sanitize the filename by removing invalid characters and truncating if necessary.

        Args:
            filename (str): The original filename.

        Returns:
            str: The sanitized filename.
        """
        # Remove invalid filename characters
        sanitized: str = re.sub(r"[^\w\-_\. ]", "_", filename)
        cleaned: str = sanitized.lstrip("_")
        # Truncate to a reasonable length (e.g., 50 characters)
        return cleaned[:100]

    def _hash_url(self, url: str) -> str:
        """
        Create a short hash of the URL.

        Args:
            url (str): The URL to hash.

        Returns:
            str: A short hash of the URL.
        """
        return str(hash(url))[-8:]  # Use last 8 digits of the hash

    def merge_pdfs(self) -> None:
        merger = PdfMerger()
        pdf_files: List[str] = [
            f for f in os.listdir(self.output_dir) if f.endswith(".pdf")
        ]
        pdf_files.sort()

        for pdf in pdf_files:
            file_path: str = os.path.join(self.output_dir, pdf)
            merger.append(file_path)

        output_path: str = os.path.join(self.output_dir, "merged_output.pdf")
        merger.write(output_path)
        merger.close()

        logging.info(f"All PDFs merged into: {output_path}")

        # Optionally, remove individual PDF files
        if self.config.get("remove_individual_pdfs", False):
            for pdf in pdf_files:
                os.remove(os.path.join(self.output_dir, pdf))
            logging.info("Individual PDF files removed after merging.")

    async def run(self) -> None:
        async with async_playwright() as p:
            try:
                self.browser = await p.chromium.launch()
                if not self.browser:
                    raise Exception("Failed to launch browser")

                browser_context: BrowserContext = (
                    await self.browser.new_context(
                        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
                    )
                )

                with tqdm(
                    total=self.max_pages, desc="Webpages processed"
                ) as self.pbar:
                    page: Page = await browser_context.new_page()
                    await self.scrape_website(
                        str(self.base_url), depth=0, page=page
                    )
            except Exception as e:
                logging.error("Error initializing browser: %s", str(e))
                logging.error("Error details:", exc_info=True)
            finally:
                if self.browser:
                    await self.browser.close()
                logging.info(
                    "Finished scraping. Total pages processed: %d",
                    self.pages_processed,
                )
                if self.error_count > 0:
                    logging.info(
                        "Successful conversions: %d", self.success_count
                    )
                    logging.info("Errors encountered: %d", self.error_count)
                    if self.errored_urls:
                        logging.info("Errored URLs:")
                        for idx, err_url in enumerate(self.errored_urls, 1):
                            logging.info("  %d. %s", idx, err_url)

                if self.config.get("merge", False):
                    self.merge_pdfs()

    def update_progress(self) -> None:
        """Update the progress bar."""
        self.pbar.n = self.pages_processed
        self.pbar.refresh()


def handle_signal(signum: int, frame: Any) -> None:
    """
    Handle termination signals.

    Args:
        signum (int): The signal number.
        frame (Any): The current stack frame.
    """
    logging.info("Received signal %d. Shutting down gracefully...", signum)
    asyncio.create_task(force_exit(5))


async def force_exit(delay: int) -> NoReturn:
    """
    Force exit the program after a specified delay.

    Args:
        delay (int): The delay in seconds before exiting.
    """
    logging.warning("Forcing exit in %d seconds...", delay)
    await asyncio.sleep(delay)
    logging.warning("Force exiting now")
    os._exit(1)


def parse_arguments() -> argparse.Namespace:
    parser: argparse.ArgumentParser = argparse.ArgumentParser(
        description="Recursively scrape a website and convert pages to PDFs using Playwright."
    )
    parser.add_argument(
        "url", type=str, help="The base URL of the website to scrape."
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="pdfs",
        help="The directory to save PDFs.",
    )
    parser.add_argument(
        "--depth",
        type=int,
        default=2,
        help="The maximum depth of recursion for scraping.",
    )
    parser.add_argument(
        "--page-size",
        type=str,
        default="A4",
        help="The page size for the PDFs.",
    )
    parser.add_argument(
        "--delay",
        type=float,
        default=1.0,
        help="The delay in seconds between requests.",
    )
    parser.add_argument(
        "--retries",
        type=int,
        default=3,
        help="The number of retries for failed requests.",
    )
    parser.add_argument(
        "--concurrency",
        type=int,
        default=4,
        help="The number of concurrent requests to execute.",
    )
    parser.add_argument(
        "--max-pages",
        type=int,
        default=100,
        help="The maximum number of pages to scrape.",
    )
    parser.add_argument(
        "--verbose", action="store_true", help="Enable verbose logging"
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=60000,
        help="Timeout for page navigation in milliseconds.",
    )
    parser.add_argument(
        "--only",
        type=str,
        default="",
        help="Comma-separated list of allowed domains or URL prefixes for scraping.",
    )
    parser.add_argument(
        "--merge",
        action="store_true",
        help="Merge all created PDFs into a single file",
    )
    parser.add_argument(
        "--exclude-url",
        type=str,
        default="",
        help="Comma-separated list of URLs to exclude from scraping.",
    )
    parser.add_argument(
        "--links-only",
        action="store_true",
        help="Only scrape links from the origin page without converting it to PDF",
    )
    parser.add_argument(
        "--pagination",
        action="store_true",
        help="Enable pagination handling for multi-page content",
    )
    return parser.parse_args()


def check_install_dependencies() -> None:
    """Check and install required Python packages if not already installed."""
    required_packages: List[str] = [
        "playwright",
        "aiohttp",
        "nbformat",
        "nbconvert",
        "tqdm",
        "PyPDF2",
    ]
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            print(f"Installing {package}...")
            os.system(f"{sys.executable} -m pip install {package}")
            print("Installing Playwright browsers...")
            os.system(f"{sys.executable} -m playwright install")


async def main() -> None:
    args: argparse.Namespace = parse_arguments()

    # Parse the --only argument
    allowed_domains: List[str] = []
    if args.only:
        for domain in args.only.split(","):
            domain: str = domain.strip()
            if not domain.startswith(("http://", "https://")):
                allowed_domains.append(domain)
            else:
                parsed: ParseResult = urlparse(domain)
                allowed_domains.append(parsed.netloc + parsed.path)

    config: Dict[str, Union[str, int, bool, List[str]]] = {
        "base_url": str(args.url),
        "output_dir": args.output_dir,
        "max_depth": args.depth,
        "page_size": args.page_size,
        "delay": args.delay,
        "retries": args.retries,
        "concurrency": args.concurrency,
        "max_pages": args.max_pages,
        "timeout": args.timeout,
        "allowed_domains": allowed_domains,
        "merge": args.merge,
        "excluded_urls": (
            args.exclude_url.split(",") if args.exclude_url else []
        ),
        "links_only": (
            args.links_only if hasattr(args, "links_only") else False
        ),
        "pagination": args.pagination,
    }

    log_level: int = logging.INFO if args.verbose else logging.WARNING
    scraper: WebScraper = WebScraper(config, log_level=log_level)

    signal.signal(signal.SIGINT, handle_signal)
    signal.signal(signal.SIGTERM, handle_signal)

    try:
        await scraper.run()
    except Exception as e:
        logging.error("An error occurred: %s", str(e))
        logging.debug("Error details:", exc_info=True)
    finally:
        if scraper.error_count > 0:
            logging.info("Successful conversions: %d", scraper.success_count)
            logging.info("Errors encountered: %d", scraper.error_count)


if __name__ == "__main__":
    check_install_dependencies()
    asyncio.run(main())
